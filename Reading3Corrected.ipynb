{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ed5edb",
   "metadata": {
    "id": "1HRBtoZW0iym"
   },
   "source": [
    "<center>\n",
    "\n",
    "## <font color='darkblue'>ASTR 21100/ASTR 31200\n",
    "<center>\n",
    "\n",
    "### <font color='darkblue'>\"Computational Techniques in Astrophysics\"\n",
    "    \n",
    "<center>\n",
    "    \n",
    "### <font color='darkblue'> Reading assignment 3 questionnaire answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb5dc3",
   "metadata": {},
   "source": [
    "**1. Check everything that applies to regression approximation**\n",
    "\n",
    "Answer: \n",
    "\n",
    "* It approximates the function but not matching training values precisely\n",
    "\n",
    "\n",
    "* Makes sense to use when measurement uncertainties in the training values are substantial\n",
    "\n",
    "\n",
    "* Polynomial approximation can use polynomial of low order even if the number of training points is large\n",
    "\n",
    "\n",
    "The following do not apply, because matching training values to machine precision is *interpolation* not regression, while it is never safe to extrapolate for ***any*** approximation method. Approximations are only safe to use if they are well constructed (for example, avoid issues such as oscillations in the polynomial approximation) and only within the range of variable $x$ where training values were available. \n",
    "\n",
    "* It approximates the function matching training values to machine precision\n",
    "\n",
    "* It is generally safer to extrapolate approximation obtained using regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9376127",
   "metadata": {},
   "source": [
    "**2. If you have a choice at what x values training values of function f(x) are evaluated to construct approximation. Which choice of x values would result in a more accurate approximation for the same polynomial order**\n",
    "\n",
    "Answer: Chebyshev nodes rescaled to approximation interval\n",
    "\n",
    "Because Chebyshev nodes are optimal from the standpoint of approximation accuracy. This is a consequence of a mathematical theorem as discussed in the notebook 05.\n",
    "\n",
    "\n",
    "**3. Check everything that applies to spline interpolation**\n",
    "\n",
    "Answer: \n",
    "\n",
    "* Spline approximation does not run into numerical issues regardless of the number of training points\n",
    "\n",
    "This is because splines use low-order polynomial defined locally. Solving for its coefficients does not run into numerical issues due to small number of simple algebraic operations involved. \n",
    "\n",
    "\n",
    "* Spline approximation is guaranteed to be smooth\n",
    "\n",
    "By construction via constraints on continuinty of $s^\\prime(x)$ and $s^{\\prime\\prime}(x)$. \n",
    "\n",
    "\n",
    "* Once we have cubic spline approximation, we also get approximations for the 1st, 2nd and 3rd derivatives\n",
    "\n",
    "Because approximation of $f(x)$ is $s(x) = a_i + b_ix + c_ix^2 + d_ix^3$ in each interval. Hence we have also approximation for $f^\\prime$ in the form of $s^\\prime = b_i + 2c_i x + 3d_i x^2$ and for 2nd derivative: $s^{\\prime\\prime} = 2c_i + 6d_i x$, and for the 3rd: $s^{\\prime\\prime\\prime} = 6d_i$. \n",
    "\n",
    "\n",
    "* Once we have cubic spline approximation, we also get approximation for the function integral\n",
    "\n",
    "Similarly to the derivatives, integral of $s(x)$ is also easily computable. So once we have $s(x)$ as approximation for $f(x)$ we also have approximation for its integral. \n",
    "\n",
    "\n",
    "**4. What approximation method would you chose as optimal for the training data shown in this plot (there are about 2000 points)? The locations of x training values cannot be changed and approximation should match function values to machine precision.**\n",
    "\n",
    "Answer: Cubic piecewise spline interpolation\n",
    "\n",
    "The reasons the other answers do not apply: \n",
    "\n",
    "Polynomial interpolation with polynomial of order N-1 where N is number of training points: the order is too high and numerical issues are likely given that we cannot define training values of x_i in this case. \n",
    "\n",
    "Polynomial regression with polynomial of 500th order: regression does not guarantee that small-scale features in the function will be reproduced, while this function has small-amplitude small-scale oscillations that need to be approximated for accurate approximation. \n",
    "\n",
    "Polynomial regression with polynomial of 100th order: the same reason as previous. \n",
    "\n",
    "\n",
    "**5. What approximation method would you chose as optimal for the training data shown in this plot? The locations of x training values cannot be changed.**\n",
    "\n",
    "\n",
    "Answer: Polynomial regression with polynomial of the 1st or 2nd order\n",
    "\n",
    "Regression is the only sensible possibility for the case when points have sizeable uncertainties (in this plot represented by \"errorbars\"). The scatter is large and there is no obvious preferrable functional form of the underlying trend except for linear or quadratic function. \n",
    "\n",
    "\n",
    "**6. What approximation method would you chose as optimal for the training data shown in this plot? The locations of x training values cannot be changed.**\n",
    "\n",
    "Answer: Cubic spline\n",
    "\n",
    "Polynomial approximation will not work here because 1) high order polynomial will be needed because function has sharp features and also small-scale waves but 2) it won't be possible to reliably compute its coefficients because we cannot choose training point locations. \n",
    "\n",
    "**7. This question is for graduate students: you have function that produced the discrete values of f(x_i) at points x_i shown below. What approximation method would you chose as optimal for the function shown in this plot? The locations of x training values can be chosen by you.** \n",
    "\n",
    "\n",
    "Answer: Cubic spline interpolation\n",
    "\n",
    "Polynomial approximation is unlikely to give a good approximation here because function has a discontinuity in the first derivative in the middle, even for Chebyshev placement of training points. In fact, this is one of the \"Runge functions\" which Runge used to illustrate large oscillations in the global polynomial approximation. \n",
    "\n",
    "Spline approximation, however, also will not be very accurate in the region of discontinuity because such discontinuities cannot be well approximated by low-order polynomials. This inaccuracy is local and is likely to be smaller than large oscillations that will be present due to discontinuity in the polynomial approximation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
