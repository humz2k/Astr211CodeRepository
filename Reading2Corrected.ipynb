{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b335528",
   "metadata": {
    "id": "1HRBtoZW0iym"
   },
   "source": [
    "<center>\n",
    "\n",
    "## <font color='darkblue'>ASTR 21100/ASTR 31200\n",
    "<center>\n",
    "\n",
    "### <font color='darkblue'>\"Computational Techniques in Astrophysics\"\n",
    "    \n",
    "<center>\n",
    "    \n",
    "### <font color='darkblue'> Reading assignment 2 questionnaire answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ed9fe",
   "metadata": {},
   "source": [
    "**1. What is the main idea of the Richardson's \"extrapolation to the limit\"?**\n",
    "\n",
    "Answer: Exploit knowledge that terms of the truncation error are proportional to a given power of step h, cancel the leading term using two estimates of derivative. \n",
    "\n",
    "We cannot cancel out of the terms in general, so the last option is out, while just decreasing step size does not change the order of accuracy of a scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a6be5",
   "metadata": {},
   "source": [
    "**2. How many times we need to evaluate function f(x) at different x values to get a 2nd order accurate estimate of the first derivative?**\n",
    "\n",
    "Answer: 2, because we can get 2nd order of magnitude estimate of $f^\\prime(x)$ using: \n",
    "\n",
    "$$\\frac{f(x+h)-f(x-h)}{2h}$$\n",
    "\n",
    "\n",
    "**3. Can an integral be numerically evaluated using trapezoidal method to machine precision accuracy while using only 2 intervals (3 function evaluations)?**\n",
    "\n",
    "Answer: yes, as shown in the section \"Why does the fractional error of the trapezoidal integration scale as $\\propto h^2$?\n",
    "Or does it always?\" it is possible to get exact answer for integrals of certain functions. These are functions for which odd derivatives at the integration limits are equal to each other. Periodic functions are such functions. \n",
    "\n",
    "**4. When we say result can be computed \"with machine precision\" in Python or other languages we mean that the fractional error of the result is approximately**\n",
    "\n",
    "Answer: $1/2^{52}\\approx$ 2e-16. This is determined by how small a number we can add or subtract to 1 and still get a result different from 1. This is determined by the 52 bits allocated to mantissa of a flating point number. \n",
    "\n",
    "\n",
    "**5. In the 04 notebook it is shown that for eps<2e-16: 1.+eps == 1  evaluates to True, which means that computer cannot differentiate between 1. and 1.+eps, for eps<1./2**52, but it also shows that Python had no problem accurately calculating difference 1.1e-16-1.05e-16. Given what you know (or based on what you read in the notebook about bit representation of floating point numbers), think about and try to provide a brief explanation for why such operation can be performed accurately,  while 1+eps, where eps<2e-16, cannot. **\n",
    "\n",
    "\n",
    "Answer: 1+eps == 1 evaluates to True for eps < 1/2^52 because exponent of the answer is 0 given that number is just slightly larger than 1, so we can only use the 52 bits of the mantissa to represent a number different from exactly 1.0. The smallest number that can be represented using mantissa has 51 0s in the leftmost bits, and 1 in the rightmost bins and is equal to $1/2^{52}$. Smaller numbers cannot be represented when exponent bits are not available to reflect smallness of the number and so when we add eps < 1/2^52 to 1, it is indistinguishable from 1.0. \n",
    "\n",
    "When we add or substract numbers comparable in magnitude, such as 1.1e-16-1.05e-16 representing result (5e-18) is not a problem because exponent bits can be used to represent the exponent of the result and it can be smaller than $1/2^{52}$. \n",
    "\n",
    "In general when floating point numbers are represented by 64 bits, the arithmetic operations on numbers that have fractional difference smaller than $1/2^{52}\\approx 2\\times 10^{-16}$. In other words, for two numbers $y<x$, when $\\vert y-x\\vert/x<1/2^{52}$ arithmetic operations involving $x$ and $y$ will have large error of order $y$ itself.\n",
    "\n",
    "\n",
    "**6. Which of these functions can be represent *exactly* only by the first 3 terms of the Taylor series expansion? **\n",
    "\n",
    "Answer: \n",
    "\n",
    "        a*x**2 + b*x + c  \n",
    "        \n",
    "because derivatives beyond 3rd order are all zero. \n",
    "\n",
    "**7. How does fractional error of an integral evaluated with trapezoidal method scale with step size of integration h?** \n",
    "\n",
    "\n",
    "Answer: as shown in the 04 notebook, the fractional error scales as $\\propto h^2$, so trapezoidal method is *2nd order accurate*. \n",
    "\n",
    "\n",
    "**8. Why do you think it is important to know how fractional accuracy of a numerical estimate scales with h? (check all that applies)**\n",
    "\n",
    "Answer: \n",
    "\n",
    "* It gives us information that allows to control accuracy of numerical integration\n",
    "\n",
    "* We can use this knowledge to achieve a much higher accuracy with combining trapezoidal estimates -> Romberg integration\n",
    "\n",
    "* It determines the \"order\" of the scheme\n",
    "\n",
    "\n",
    "This, unfortunately does not apply: \n",
    "It allows us to potentially compute integral with arbitrarily small error\n",
    "\n",
    "because roundoff error due to limited number of bits, ultimately limits how accurately we can estimate something. \n",
    "\n",
    "\n",
    "**9. This question is for graduate students: would the approach used in the Romberg method work if truncation terms of a scheme would look like e1*h + e2*h^2 + e3*h^3 +... as opposed to having only even terms as in the trapezoidal scheme?**\n",
    "\n",
    "\n",
    "Answer: yes, we can still cancel leading term, but the gain would not be as large, because instead of gaining 2 orders of accuracy with each cancellation, we will only gain one.\n",
    "\n",
    "\n",
    "**10. This question is for graduate students: why do we generally need to consider both absolute and relative error tolerances?** \n",
    "\n",
    "Answer: when the result is expected to be zero or close to zero, computing fractional (relative) error may involve division by 0 and so it may not be possible to compute. Also, we simply may be interested in the absolute accuracy even if we don't know the magnitude of the ultimate result. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
